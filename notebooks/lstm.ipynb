{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd584f0-2f32-424e-9339-0f52b29e9cef",
   "metadata": {},
   "source": [
    "# Music Generation using LSTM neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f809649-edbb-42b5-83e3-fe7bd1ce54f9",
   "metadata": {},
   "source": [
    "This notebook walk through the process of building Music Generation model based on LSTM using Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb33fdc-8107-4500-ad91-f4b03289fe35",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3982c34-7555-4da9-8d2e-f34d37027fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import music21 as m21\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c1d6ec-08e9-44c6-8114-d9ee7c109dc7",
   "metadata": {},
   "source": [
    "## Preprocessing song dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dee012-37e5-443e-8cab-e822b2feda0b",
   "metadata": {},
   "source": [
    "I'm using different piano melodies in MIDI format mostly from Final Fantasy soundtracks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7746311c-d296-4a6d-b183-7b5841b9bbdc",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fb4b068-5c6f-43fc-a382-eb69353779eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86936645-832a-408f-aced-9d2d7d0bdc7e",
   "metadata": {},
   "source": [
    "Here you can see the code that parse all MIDI files and makes array with all the notes and chords from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fe3454f-3260-4e07-a46e-bbdc7d74e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = []\n",
    "\n",
    "for file in glob.glob(DATA_PATH + '*.mid'):\n",
    "    midi = m21.converter.parse(file)\n",
    "    notes_to_parse = None\n",
    "\n",
    "    parts = m21.instrument.partitionByInstrument(midi)\n",
    "    if parts:\n",
    "        notes_to_parse = parts.parts[0].recurse()\n",
    "    else:\n",
    "        notes_to_parse = midi.flat.notes\n",
    "\n",
    "    for element in notes_to_parse:\n",
    "        if isinstance(element, m21.note.Note):\n",
    "            notes.append(str(element.pitch))\n",
    "        elif isinstance(element, m21.chord.Chord):\n",
    "            notes.append('.'.join(str(n) for n in element.normalOrder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a5878-4cf5-4437-98a3-40b237cc3dbd",
   "metadata": {},
   "source": [
    "### Shaping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30069892-8c19-4a51-8788-cb2013871958",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf5c290-b535-4907-bffc-992fa80c6d61",
   "metadata": {},
   "source": [
    "Get all unique pitchnames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee765218-1fcc-4790-84bb-25d58ec31c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitchnames = sorted(set(notes))\n",
    "n_vocab = len(pitchnames)\n",
    "n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368995d8-c491-4890-85ae-4643ad9ed729",
   "metadata": {},
   "source": [
    "Create a note_to_int dictionary to map pitches to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f2a9c13-b656-4662-b1b6-4646da9a8084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_to_int = {note:number for number, note in enumerate(pitchnames)}\n",
    "note_to_int[pitchnames[52]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae755e5-0066-45b5-8671-c0e786962e7a",
   "metadata": {},
   "source": [
    "Create input sequences and the corresponding outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3cbcc10e-9fb0-48ac-93ab-b639d354d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input = []\n",
    "network_output = []\n",
    "\n",
    "for i in range(0, len(notes) - SEQ_LEN):\n",
    "    input_seq = notes[i:i + SEQ_LEN]\n",
    "    output_note = notes[i + SEQ_LEN]\n",
    "    network_input.append([note_to_int[char] for char in input_seq])\n",
    "    network_output.append(note_to_int[output_note])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96cef5-4cf1-4d13-81ea-68014a26f0ca",
   "metadata": {},
   "source": [
    "Reshape the input into a format compatible with LSTM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f5e69e9-a00b-494e-8f03-9066231a5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input = np.reshape(network_input, (len(network_input), SEQ_LEN, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e05e71-fb78-454c-b845-a842baab5a3d",
   "metadata": {},
   "source": [
    "Normalize input and one-hot encode the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "41e0b134-982a-4992-a413-987bb1c58594",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input = network_input / float(n_vocab)\n",
    "network_output = tf.keras.utils.to_categorical(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be9911b6-6d8f-4c36-a32c-0ae9bb253f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45876, 100, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581de2d-e51e-40ac-9d41-950483f854d2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf285358-767d-4f91-9c91-cdedccf58469",
   "metadata": {},
   "source": [
    "### Neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ddc11-28a9-4c4d-b3fd-3494370b4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    512,\n",
    "    input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "    return_sequences=True\n",
    "))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173d3d04-f73d-4566-8b8c-974d414a11bb",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94326526-cffa-423f-aa4b-c2ce45c430dd",
   "metadata": {},
   "source": [
    "Declare checkpoints to save the weights and be able to stop training at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2655c6-f461-4fa9-8491-3db261037d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"    \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='loss', \n",
    "    verbose=0,        \n",
    "    save_best_only=True,        \n",
    "    mode='min'\n",
    ")    \n",
    "callbacks_list = [checkpoint]     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b130ec-0f01-41b7-a71b-1d3a830cc5e1",
   "metadata": {},
   "source": [
    "Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca185b8-9936-485a-9545-7bf36ec90e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a2dba-e975-4da5-abe6-e7149965d814",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2c95f-1810-40ef-8029-09bbf10d67ed",
   "metadata": {},
   "source": [
    "Set up the network model in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e3dfa-e8d7-4ca7-9bf5-e46e2e0cd69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    512,\n",
    "    input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "    return_sequences=True\n",
    "))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc441c-4550-45f4-b517-a0fa42fd2abb",
   "metadata": {},
   "source": [
    "Load the weights to each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25c144-e337-430f-9b33-b5ae29c5bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66474867-4928-4cd7-9b8b-920e1b0bc197",
   "metadata": {},
   "source": [
    "Randomize the first note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd7eb6-0489-4cf7-bfa8-e6a04aaebc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = numpy.random.randint(0, len(network_input)-1)\n",
    "\n",
    "pattern = network_input[start]\n",
    "prediction_output = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ca651-2fe9-41ba-8580-5f84d45cda46",
   "metadata": {},
   "source": [
    "Create a int_to_note dictionary to map integers to pitches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d56a206-fa25-4f7d-a86d-2256d4d05d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_note = {number:note for number, note in enumerate(pitchnames)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6ad98-6187-436c-9aa2-626bba08d141",
   "metadata": {},
   "source": [
    "Generate 500 notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711eee03-fa47-4442-8bc2-566f95f5d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for note_index in range(500):\n",
    "    prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    prediction_input = prediction_input / float(n_vocab)\n",
    "    prediction = model.predict(prediction_input, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_note[index]\n",
    "    prediction_output.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
